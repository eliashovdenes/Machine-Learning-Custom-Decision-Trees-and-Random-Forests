{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 â€“ Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Reload all modules without having to restart the kernel\n",
    "# Useful for development if you have edited any of the external code files.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# My implementations\n",
    "from decision_tree import DecisionTree\n",
    "from random_forest import RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Do data loading, exploration and preprocessing as you see fit.\n",
    "\n",
    "Here is some code to load the dataset to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns names: ['citric_acid', 'residual_sugar', 'pH', 'sulphates', 'alcohol']\n",
      "Target column name: type\n",
      "X shape: (500, 5)\n",
      "y shape: (500,)\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"wine_dataset_small.csv\", delimiter=\",\", dtype=float, names=True)\n",
    "\n",
    "feature_names = list(data.dtype.names[:-1])\n",
    "target_name = data.dtype.names[-1]\n",
    "\n",
    "X = np.array([data[feature] for feature in feature_names]).T\n",
    "y = data[target_name].astype(int)\n",
    "\n",
    "print(f\"Feature columns names: {feature_names}\")\n",
    "print(f\"Target column name: {target_name}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Set Random Seeds. Create a requirements.txt. Document Hyperparameters. Use Version Control. '"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which hyperparameters should you tune?\n",
    "\n",
    "\"\"\" I think the max_depth, n_estimators(how many trees), criterion, max_features \"\"\"\n",
    "\n",
    "# Which values should you test for each hyperparameter?\n",
    "\n",
    "\"\"\"max_depth: 1 to 15?,    n_estimators: 1 to 200,   criterion: entropy or gini,  max_features: sqrt or log2 or None\"\"\"\n",
    "\n",
    "# Which model selection method should you use (e.g., hold-out validation, k-fold cross-validation)?\n",
    "\n",
    "\"\"\"k-fold cross-validation: This is ideal because it splits the dataset into k subsets (e.g., k=5), trains the model on k-1 folds, and tests on the remaining fold. The process is repeated k times with different folds. This helps to reduce the risk of overfitting to a particular subset of data.\n",
    "\n",
    "Why: It ensures that your model isn't overly tuned to any specific training or testing split and provides a more generalized measure of performance.\"\"\"\n",
    "\n",
    "# Which performance measure should you use for model selection (e.g., accuracy, F1-score)?\n",
    "\n",
    "\"\"\"Accuracy is fine becuase i think that there wont be class imbalance in the classes, if it were we should use F1-score. Try both\"\"\"\n",
    "\n",
    "# How do you ensure that your model selection process is fair and unbiased?\n",
    "\n",
    "\"\"\"follow best practices in data splitting, hyperparameter tuning, and evaluation. Avoid Data Leakage. Avoid Overfitting. Ensure Class Balance \"\"\"\n",
    "\n",
    "# How can you ensure reproducibility of your results?\n",
    "\n",
    "\"\"\"Set Random Seeds. Create a requirements.txt. Document Hyperparameters. Use Version Control. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed \n",
    "seed = 0\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "#Split the data into test and (training and validation)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X,y, test_size=0.3, random_state=seed, shuffle=True)\n",
    "\n",
    "# Use k fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set list of the hyperparameters that i want to tune\n",
    "\n",
    "max_depth_params = [3,10,20,None]\n",
    "\n",
    "n_estimators = [2,10]\n",
    "\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "\n",
    "max_features = [\"sqrt\", \"log2\", None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for tuning random forest using k-fold cross validation\n",
    "\n",
    "def tune_random_forest(model_type, X_train_val, y_train_val, n_estimators, max_depth_params, criterion, max_features, kf):\n",
    "    best_accuracy = [0]  # To store the best average accuracy and the corresponding hyperparameters\n",
    "    \n",
    "    # Iterate through all combinations of hyperparameters\n",
    "    for maxdp in max_depth_params:\n",
    "        for n_est in n_estimators:\n",
    "            for crit in criterion:\n",
    "                for mf in max_features:\n",
    "                    \n",
    "                    # Instantiate the model based on the input string\n",
    "                    if model_type == \"RandomForest\":\n",
    "                        rf = RandomForest(n_estimators=n_est, max_depth=maxdp, criterion=crit, max_features=mf)\n",
    "                    elif model_type == \"SklearnRandomForest\":\n",
    "                        rf = RandomForestClassifier(n_estimators=n_est, max_depth=maxdp, criterion=crit, max_features=mf, random_state=42)\n",
    "                    else:\n",
    "                        raise ValueError(\"Invalid model_type. Choose either 'RandomForest' or 'SklearnRandomForest'.\")\n",
    "                    \n",
    "                    # List to store accuracies for each fold\n",
    "                    fold_accuracies = []\n",
    "                    \n",
    "                    # Perform k-fold cross-validation\n",
    "                    for train_index, val_index in kf.split(X_train_val):\n",
    "                        # Split the data into training and validation sets for each fold\n",
    "                        X_training, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "                        y_training, y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "                        \n",
    "                        # Train the model on the training set\n",
    "                        rf.fit(X_training, y_training)\n",
    "                        \n",
    "                        # Predict on the validation set\n",
    "                        y_pred = rf.predict(X_val)\n",
    "                        \n",
    "                        # Calculate accuracy for the current fold\n",
    "                        accuracy = accuracy_score(y_val, y_pred)\n",
    "                        fold_accuracies.append(accuracy)\n",
    "                    \n",
    "                    # Calculate average accuracy across all folds\n",
    "                    average_accuracy = np.mean(fold_accuracies)\n",
    "                    \n",
    "                    # Update best accuracy and hyperparameters if current average is better\n",
    "                    if average_accuracy > best_accuracy[0]:\n",
    "                        best_accuracy = [average_accuracy, n_est, maxdp, crit, mf]\n",
    "    \n",
    "    # Return the best hyperparameters and accuracy\n",
    "    return best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.96), 10, 10, 'gini', 'sqrt']\n",
      "The best average accuracy is 0.96 with N_estimators: 10, Max_depth: 10, Criterion: gini, Max_features: sqrt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find best values for hyperparameters for my own random forest classifier\n",
    "\n",
    "best_hypermeters_random_forest = tune_random_forest(\"RandomForest\",X_train_val, y_train_val, n_estimators, max_depth_params, criterion, max_features, kf)\n",
    "\n",
    "\n",
    "print(best_hypermeters_random_forest)\n",
    "\n",
    "print(f\"The best average accuracy is {best_hypermeters_random_forest[0]} with N_estimators: {best_hypermeters_random_forest[1]}, Max_depth: {best_hypermeters_random_forest[2]}, Criterion: {best_hypermeters_random_forest[3]}, Max_features: {best_hypermeters_random_forest[4]}.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation accuracy: 0.9942857142857143\n",
      "Test accuracy: 0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "# Testing the hyperparameters on test set with my own random forest classifier\n",
    "\n",
    "rf = RandomForest(n_estimators=best_hypermeters_random_forest[1], max_depth=best_hypermeters_random_forest[2], criterion=best_hypermeters_random_forest[3], max_features=best_hypermeters_random_forest[4], seed=seed)\n",
    "\n",
    "rf.fit(X_train_val,y_train_val)\n",
    "\n",
    "print(f\"Training and validation accuracy: {accuracy_score(y_train_val, rf.predict(X_train_val))}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, rf.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.877142857142857), 10, 10, 'gini', 'sqrt']\n",
      "The best average accuracy is 0.877142857142857 with N_estimators: 10, Max_depth: 10, Criterion: gini, Max_features: sqrt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find best values for hyperparameters for my sklearn random forest classifier\n",
    "\n",
    "best_hypermeters_random_forest_sklearn = tune_random_forest(\"SklearnRandomForest\",X_train_val, y_train_val, n_estimators, max_depth_params, criterion, max_features, kf)\n",
    "\n",
    "print(best_hypermeters_random_forest_sklearn)\n",
    "\n",
    "print(f\"The best average accuracy is {best_hypermeters_random_forest_sklearn [0]} with N_estimators: {best_hypermeters_random_forest_sklearn [1]}, Max_depth: {best_hypermeters_random_forest_sklearn [2]}, Criterion: {best_hypermeters_random_forest_sklearn [3]}, Max_features: {best_hypermeters_random_forest_sklearn [4]}.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation accuracy: 0.9857142857142858\n",
      "Test accuracy: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Testing the hyperparameters on test set\n",
    "rf = RandomForestClassifier(n_estimators=best_hypermeters_random_forest_sklearn[1], max_depth=best_hypermeters_random_forest_sklearn[2], criterion=best_hypermeters_random_forest_sklearn[3], max_features=best_hypermeters_random_forest_sklearn[4], random_state=seed)\n",
    "\n",
    "rf.fit(X_train_val,y_train_val)\n",
    "\n",
    "print(f\"Training and validation accuracy: {accuracy_score(y_train_val, rf.predict(X_train_val))}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, rf.predict(X_test))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF264",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
