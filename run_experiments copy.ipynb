{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 â€“ Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Reload all modules without having to restart the kernel\n",
    "# Useful for development if you have edited any of the external code files.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# My implementations\n",
    "from decision_tree import DecisionTree\n",
    "from random_forest import RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Do data loading, exploration and preprocessing as you see fit.\n",
    "\n",
    "Here is some code to load the dataset to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns names: ['citric_acid', 'residual_sugar', 'pH', 'sulphates', 'alcohol']\n",
      "Target column name: type\n",
      "X shape: (500, 5)\n",
      "y shape: (500,)\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"wine_dataset_small.csv\", delimiter=\",\", dtype=float, names=True)\n",
    "\n",
    "feature_names = list(data.dtype.names[:-1])\n",
    "target_name = data.dtype.names[-1]\n",
    "\n",
    "X = np.array([data[feature] for feature in feature_names]).T\n",
    "y = data[target_name].astype(int)\n",
    "\n",
    "print(f\"Feature columns names: {feature_names}\")\n",
    "print(f\"Target column name: {target_name}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Set Random Seeds. Create a requirements.txt. Document Hyperparameters. Use Version Control. '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which hyperparameters should you tune?\n",
    "\n",
    "\"\"\" I think the max_depth, n_estimators(how many trees), criterion, max_features \"\"\"\n",
    "\n",
    "# Which values should you test for each hyperparameter?\n",
    "\n",
    "\"\"\"max_depth: 1 to 15?,    n_estimators: 1 to 200,   criterion: entropy or gini,  max_features: sqrt or log2 or None\"\"\"\n",
    "\n",
    "# Which model selection method should you use (e.g., hold-out validation, k-fold cross-validation)?\n",
    "\n",
    "\"\"\"k-fold cross-validation: This is ideal because it splits the dataset into k subsets (e.g., k=5), trains the model on k-1 folds, and tests on the remaining fold. The process is repeated k times with different folds. This helps to reduce the risk of overfitting to a particular subset of data.\n",
    "\n",
    "Why: It ensures that your model isn't overly tuned to any specific training or testing split and provides a more generalized measure of performance.\"\"\"\n",
    "\n",
    "# Which performance measure should you use for model selection (e.g., accuracy, F1-score)?\n",
    "\n",
    "\"\"\"Accuracy is fine becuase i think that there wont be class imbalance in the classes, if it were we should use F1-score. Try both\"\"\"\n",
    "\n",
    "# How do you ensure that your model selection process is fair and unbiased?\n",
    "\n",
    "\"\"\"follow best practices in data splitting, hyperparameter tuning, and evaluation. Avoid Data Leakage. Avoid Overfitting. Ensure Class Balance \"\"\"\n",
    "\n",
    "# How can you ensure reproducibility of your results?\n",
    "\n",
    "\"\"\"Set Random Seeds. Create a requirements.txt. Document Hyperparameters. Use Version Control. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed \n",
    "seed = 0\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "#Split the data\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X,y, test_size=0.3, random_state=seed, shuffle=True)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.9628571428571429), 100, 20, 'entropy', 'sqrt']\n",
      "\n",
      "\n",
      "The best accuracy is 0.9628571428571429 with N_estimators: 100. Max_depth: 20. Criterion: entropy. Max_features: sqrt.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find best values for hyperparameters\n",
    "\n",
    "max_depth_params = [3,10,15,20,25,None]\n",
    "\n",
    "n_estimators =[2,4,10,20,50,100]\n",
    "\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "\n",
    "max_features = [\"sqrt\", \"log2\", None]\n",
    "\n",
    "best_accuracy = [[0]]\n",
    "\n",
    "for maxdp in max_depth_params:\n",
    "\n",
    "    for n_est in n_estimators:\n",
    "\n",
    "        for crit in criterion:\n",
    "\n",
    "            for mf in max_features:\n",
    "\n",
    "                rf = RandomForest(n_estimators=n_est, max_depth=maxdp, criterion=crit, max_features=mf)\n",
    "\n",
    "                fold_accuracies = []\n",
    "                for train_index, val_index in kf.split(X_train_val):\n",
    "                    \n",
    "                    X_training, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "                    y_training, y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "                    \n",
    "                    rf.fit(X_training, y_training)\n",
    "                    \n",
    "                    \n",
    "                    y_pred = rf.predict(X_val)\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_val, y_pred)\n",
    "                    fold_accuracies.append(accuracy)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                # Average accuracy across all folds\n",
    "                average_accuracy = np.mean(fold_accuracies)\n",
    "\n",
    "                if average_accuracy > best_accuracy[0]:\n",
    "                    best_accuracy = [average_accuracy, n_est, maxdp, crit, mf]\n",
    "                # print(f\"Average accuracy: {average_accuracy}. N_estimators: {n_est}. Max_features: {mf}. Criterion: {crit}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(best_accuracy)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"The best average accuracy is {best_accuracy[0]} with N_estimators: {best_accuracy[1]}. Max_depth: {best_accuracy[2]}. Criterion: {best_accuracy[3]}. Max_features: {best_accuracy[4]}.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation accuracy: 0.94\n",
      "Test accuracy: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Testing the hyperparameters on test set\n",
    "\n",
    "rf = RandomForest(n_estimators=best_accuracy[1], max_depth=best_accuracy[2], criterion=best_accuracy[3], max_features=best_accuracy[4])\n",
    "\n",
    "rf.fit(X_train_val,y_train_val)\n",
    "\n",
    "print(f\"Training and validation accuracy: {accuracy_score(y_train_val, rf.predict(X_train_val))}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, rf.predict(X_test))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.8914285714285715), 50, 25, 'gini', 'log2']\n",
      "\n",
      "\n",
      "The best average accuracy is 0.8914285714285715 with N_estimators: 50. Max_depth: 25. Criterion: gini. Max_features: log2.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding hyperparameters for sklearns randomforestclassifier\n",
    "\n",
    "max_depth_params = [3,10,15,20,25,None]\n",
    "\n",
    "n_estimators =[2,4,10,20,50,100]\n",
    "\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "\n",
    "max_features = [\"sqrt\", \"log2\", None]\n",
    "\n",
    "best_accuracy = [[0]]\n",
    "\n",
    "for maxdp in max_depth_params:\n",
    "\n",
    "    for n_est in n_estimators:\n",
    "\n",
    "        for crit in criterion:\n",
    "\n",
    "            for mf in max_features:\n",
    "\n",
    "                rf = RandomForestClassifier(n_estimators=n_est, max_depth=maxdp, criterion=crit, max_features=mf)\n",
    "\n",
    "                fold_accuracies = []\n",
    "                for train_index, val_index in kf.split(X_train_val):\n",
    "                    \n",
    "                    X_training, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "                    y_training, y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "                    \n",
    "                    rf.fit(X_training, y_training)\n",
    "                    \n",
    "                    \n",
    "                    y_pred = rf.predict(X_val)\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_val, y_pred)\n",
    "                    fold_accuracies.append(accuracy)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                # Average accuracy across all folds\n",
    "                average_accuracy = np.mean(fold_accuracies)\n",
    "\n",
    "                if average_accuracy > best_accuracy[0]:\n",
    "                    best_accuracy = [average_accuracy, n_est, maxdp, crit, mf]\n",
    "                # print(f\"Average accuracy: {average_accuracy}. N_estimators: {n_est}. Max_features: {mf}. Criterion: {crit}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(best_accuracy)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"The best average accuracy for sklearn is {best_accuracy[0]} with N_estimators: {best_accuracy[1]}. Max_depth: {best_accuracy[2]}. Criterion: {best_accuracy[3]}. Max_features: {best_accuracy[4]}.\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation accuracy: 1.0\n",
      "Test accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Testing the hyperparameters on test set\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=best_accuracy[1], max_depth=best_accuracy[2], criterion=best_accuracy[3], max_features=best_accuracy[4])\n",
    "\n",
    "rf.fit(X_train_val,y_train_val)\n",
    "\n",
    "print(f\"Training and validation accuracy: {accuracy_score(y_train_val, rf.predict(X_train_val))}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, rf.predict(X_test))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF264",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
